{"config":{"indexing":"full","lang":["nl"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]"},"docs":[{"location":"","text":"Personal Knowledge Management \u00b6 Waarom een PKM? \u00b6 We hebben deze PKM opgezet aangezien we beide dagelijks veel bijleren en er dagelijks nieuwe technologi\u00eben en ontwikkelingen ontstaan. Aangezien we het fijn vinden om mee te zijn met de laatste nieuwtjes, hebben we uiteraard ook een plaats nodig om dit te kunnen documenteren. Hoe is deze PKM ontstaan? \u00b6 In het begin hebben we op Discord veel nuttige informatie \"\ud83d\udccc gepind\" in onze gezamelijke chat. Deze chat is heilig voor ons en hebben al vaak gesproken over wat een ramp het zou zijn als deze chat ooit verwijderd zou worden. Uiteraard is een chat niet overzichtelijk en moeten we toch vaak beroep doen op de zoekfunctie. Daarom heeft Bedar voorgesteld om te werken met mkdocs . Welke informatie kan ik hier terugvinden? \u00b6 Alle informatie die we belangrijk genoeg vinden om te documenteren en die ons kunnen helpen met onze dagelijkse taken. Mag ik zelf iets toevoegen of verbeteren? \u00b6 Uiteraard! Je bent altijd welkom om PR's te sturen. Hoe kan ik jullie contacteren? \u00b6 Je kan meer informatie over ons vinden over Bedar en Cowarol op de over Cowadar pagina.","title":"\ud83c\udfe0 Home"},{"location":"#personal-knowledge-management","text":"","title":"Personal Knowledge Management"},{"location":"#waarom-een-pkm","text":"We hebben deze PKM opgezet aangezien we beide dagelijks veel bijleren en er dagelijks nieuwe technologi\u00eben en ontwikkelingen ontstaan. Aangezien we het fijn vinden om mee te zijn met de laatste nieuwtjes, hebben we uiteraard ook een plaats nodig om dit te kunnen documenteren.","title":"Waarom een PKM?"},{"location":"#hoe-is-deze-pkm-ontstaan","text":"In het begin hebben we op Discord veel nuttige informatie \"\ud83d\udccc gepind\" in onze gezamelijke chat. Deze chat is heilig voor ons en hebben al vaak gesproken over wat een ramp het zou zijn als deze chat ooit verwijderd zou worden. Uiteraard is een chat niet overzichtelijk en moeten we toch vaak beroep doen op de zoekfunctie. Daarom heeft Bedar voorgesteld om te werken met mkdocs .","title":"Hoe is deze PKM ontstaan?"},{"location":"#welke-informatie-kan-ik-hier-terugvinden","text":"Alle informatie die we belangrijk genoeg vinden om te documenteren en die ons kunnen helpen met onze dagelijkse taken.","title":"Welke informatie kan ik hier terugvinden?"},{"location":"#mag-ik-zelf-iets-toevoegen-of-verbeteren","text":"Uiteraard! Je bent altijd welkom om PR's te sturen.","title":"Mag ik zelf iets toevoegen of verbeteren?"},{"location":"#hoe-kan-ik-jullie-contacteren","text":"Je kan meer informatie over ons vinden over Bedar en Cowarol op de over Cowadar pagina.","title":"Hoe kan ik jullie contacteren?"},{"location":"about/bedar/","text":"Bedar \u00b6 Over mij \u00b6 Social links \u00b6 social.bedar.be","title":"Bedar"},{"location":"about/bedar/#bedar","text":"","title":"Bedar"},{"location":"about/bedar/#over-mij","text":"","title":"Over mij"},{"location":"about/bedar/#social-links","text":"social.bedar.be","title":"Social links"},{"location":"about/cowadar/","text":"Cowadar \u00b6 Betekenis Cowadar \u00b6 Cowadar is een samenvoeging van COWArol en beDAR . Ontstaan van Cowadar \u00b6 Cowadar is ontstaan doordat Cowarol en Bedar dezelfde interesses hebben en een plaats zochten om alles kennis te bundelen. Hierdoor kunnen we niet enkel persoonlijk groeien, maar eveneens ook op professioneel vlak. Zo is Cowarol beter in Networking en Bedar is dan weer beter in programmeren . Door deze sterktes en zwaktes te combineren en aan te vullen, kunnen samen sneller en efficienter groeien en kennis opdoen.","title":"Algemeen"},{"location":"about/cowadar/#cowadar","text":"","title":"Cowadar"},{"location":"about/cowadar/#betekenis-cowadar","text":"Cowadar is een samenvoeging van COWArol en beDAR .","title":"Betekenis Cowadar"},{"location":"about/cowadar/#ontstaan-van-cowadar","text":"Cowadar is ontstaan doordat Cowarol en Bedar dezelfde interesses hebben en een plaats zochten om alles kennis te bundelen. Hierdoor kunnen we niet enkel persoonlijk groeien, maar eveneens ook op professioneel vlak. Zo is Cowarol beter in Networking en Bedar is dan weer beter in programmeren . Door deze sterktes en zwaktes te combineren en aan te vullen, kunnen samen sneller en efficienter groeien en kennis opdoen.","title":"Ontstaan van Cowadar"},{"location":"about/cowarol/","text":"Cowarol \u00b6","title":"Cowarol"},{"location":"about/cowarol/#cowarol","text":"","title":"Cowarol"},{"location":"docker/install/","text":"Docker desktop installeren \u00b6","title":"Docker install"},{"location":"docker/install/#docker-desktop-installeren","text":"","title":"Docker desktop installeren"},{"location":"docker/services/documentation/mkdocs/","text":"","title":"mkdocs"},{"location":"docker/services/gameserver/fivem/fivem/","text":"Fivem \u00b6 Download laatste fivem artifacts \u00b6 Command \u00b6 1 wget https://runtime.fivem.net/artifacts/fivem/build_proot_linux/master/5878-a5c270439ddb3bbb1fc4e7d02cb5593be84a9b89/fx.tar.xz && tar -xf fx.tar.xz --strip-components = 1 --exclude alpine/dev --exclude alpine/proc --exclude alpine/run --exclude alpine/sys && rm fx.tar.xz Artifacts URL \u00b6 Linux Windows","title":"Fivem"},{"location":"docker/services/gameserver/fivem/fivem/#fivem","text":"","title":"Fivem"},{"location":"docker/services/gameserver/fivem/fivem/#download-laatste-fivem-artifacts","text":"","title":"Download laatste fivem artifacts"},{"location":"docker/services/gameserver/fivem/fivem/#command","text":"1 wget https://runtime.fivem.net/artifacts/fivem/build_proot_linux/master/5878-a5c270439ddb3bbb1fc4e7d02cb5593be84a9b89/fx.tar.xz && tar -xf fx.tar.xz --strip-components = 1 --exclude alpine/dev --exclude alpine/proc --exclude alpine/run --exclude alpine/sys && rm fx.tar.xz","title":"Command"},{"location":"docker/services/gameserver/fivem/fivem/#artifacts-url","text":"Linux Windows","title":"Artifacts URL"},{"location":"docker/services/gameserver/pterodactyl/pterodactyl/","text":"","title":"Pterodactyl"},{"location":"network/networking/","text":"Networking \u00b6","title":"Networking"},{"location":"network/networking/#networking","text":"","title":"Networking"},{"location":"network/dns/cloudflare/","text":"","title":"Cloudflare"},{"location":"network/dns/nginx_proxy_manager/","text":"","title":"NPM"},{"location":"network/dns/pihole/","text":"","title":"Pihole"},{"location":"network/dns/traefik/","text":"","title":"Traefik"},{"location":"network/router/pfsense/","text":"","title":"Pfsense"},{"location":"network/router/unifi/","text":"","title":"Unifi"},{"location":"network/vpn/vpn/","text":"","title":"VPN"},{"location":"operating_systems/operating_systems/","text":"Operating Systems \u00b6 Beschrijving \u00b6 Links \u00b6 Ubuntu Raspberry Pi OS","title":"Operating Systems"},{"location":"operating_systems/operating_systems/#operating-systems","text":"","title":"Operating Systems"},{"location":"operating_systems/operating_systems/#beschrijving","text":"","title":"Beschrijving"},{"location":"operating_systems/operating_systems/#links","text":"Ubuntu Raspberry Pi OS","title":"Links"},{"location":"operating_systems/linux/debian/","text":"","title":"Debian"},{"location":"operating_systems/linux/raspberry_pi_os/raspberry_pi_os/","text":"Raspberry Pi OS \u00b6","title":"Raspberry Pi OS"},{"location":"operating_systems/linux/raspberry_pi_os/raspberry_pi_os/#raspberry-pi-os","text":"","title":"Raspberry Pi OS"},{"location":"operating_systems/linux/ubuntu/docker/","text":"Docker Engine installeren op Ubuntu \u00b6 Om aan de slag te gaan met Docker Engine op Ubuntu, moet u ervoor zorgen dat u aan de vereisten voldoet en vervolgens Docker installeert . Vereisten \u00b6 OS-vereisten \u00b6 Om Docker Engine te installeren, hebt u de 64-bits versie van een van deze Ubuntu-versies nodig: Ubuntu Jammy 22.04 (LTS) Ubuntu Impish 21.10 Ubuntu Focal 20.04 (LTS) Ubuntu Bionic 18.04 (LTS) Docker Engine wordt ondersteund op x86_64 (of amd64 ), armhf , arm64 , en s390x architecturen. Oude versies verwijderen \u00b6 Oudere versies van Docker werden docker , docker.io , of docker-engine . Als deze zijn ge\u00efnstalleerd, verwijder ze dan: 1 sudo apt-get remove docker docker-engine docker.io containerd runc Het is ok\u00e9 als apt-get wordt gemeld dat geen van deze pakketten is ge\u00efnstalleerd. De inhoud van /var/lib/docker/ , inclusief afbeeldingen, containers, volumes en netwerken, blijft behouden. Als u uw bestaande gegevens niet hoeft op te slaan en met een schone installatie wilt beginnen, raadpleegt u het gedeelte Docker Engine verwijderen onderaan deze pagina. Installatiemethoden _ \u00b6 U kunt Docker Engine op verschillende manieren installeren, afhankelijk van uw behoeften: De meeste gebruikers stellen de repositories van Docker in en installeren van daaruit, voor eenvoudige installatie en upgradetaken. Dit is de aanbevolen aanpak. Sommige gebruikers downloaden het DEB-pakket en installeren het handmatig en beheren upgrades volledig handmatig. Dit is handig in situaties zoals het installeren van Docker op air-gapped systemen zonder toegang tot internet. In test- en ontwikkelomgevingen kiezen sommige gebruikers ervoor om geautomatiseerde gemaksscripts te gebruiken om Docker te installeren. Installeren met behulp van de repository \u00b6 Voordat u Docker Engine voor de eerste keer op een nieuwe hostcomputer installeert, moet u de Docker-repository instellen. Daarna kunt u Docker installeren en bijwerken vanuit de repository. De opslagplaats instellen \u00b6 Werk de apt pakketindex bij en installeer pakketten om het apt gebruik van een repository via HTTPS toe te staan: 1 2 3 4 5 6 7 $ sudo apt-get update $ sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Voeg de offici\u00eble GPG-sleutel van Docker toe: 1 2 sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg Gebruik de volgende opdracht om de repository in te stellen: 1 2 3 $ echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Docker Engine installeren \u00b6 Werk de apt pakketindex bij en installeer de nieuwste versie van Docker Engine, containerd en Docker Compose, of ga naar de volgende stap om een specifieke versie te installeren: 1 2 sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin Om een specifieke versie van Docker Engine te installeren, vermeldt u de beschikbare versies in de repo, selecteert u en installeert u: a. Maak een lijst van de beschikbare versies in uw repo: 1 2 3 4 5 6 $ apt-cache madison docker-ce docker-ce | 5 :20.10.16~3-0~ubuntu-jammy | https://download.docker.com/linux/ubuntu jammy/stable amd64 Packages docker-ce | 5 :20.10.15~3-0~ubuntu-jammy | https://download.docker.com/linux/ubuntu jammy/stable amd64 Packages docker-ce | 5 :20.10.14~3-0~ubuntu-jammy | https://download.docker.com/linux/ubuntu jammy/stable amd64 Packages docker-ce | 5 :20.10.13~3-0~ubuntu-jammy | https://download.docker.com/linux/ubuntu jammy/stable amd64 Packages b. Installeer een specifieke versie met behulp van de versiereeks uit de tweede kolom, bijvoorbeeld 5:20.10.16~3-0~ubuntu-jammy . 1 sudo apt-get install docker-ce = <VERSION_STRING> docker-ce-cli = <VERSION_STRING> containerd.io docker-compose-plugin Controleer of Docker Engine correct is ge\u00efnstalleerd door de hello-world afbeelding uit te voeren. 1 2 sudo service docker start sudo docker run hello-world Met deze opdracht wordt een testimage gedownload en uitgevoerd in een container. Wanneer de container wordt uitgevoerd, drukt deze een bericht af en wordt afgesloten. Docker Engine is ge\u00efnstalleerd en draait. De docker groep is gemaakt, maar er worden geen gebruikers aan toegevoegd. U moet gebruiken sudo om Docker-opdrachten uit te voeren. Ga door naar Linux na de installatie om niet-bevoegde gebruikers toe te staan Docker-opdrachten uit te voeren en voor andere optionele configuratiestappen. Docker-engine upgraden \u00b6 Om Docker Engine te upgraden, voert u eerst uit sudo apt-get update , volgt u de installatie-instructies en kiest u de nieuwe versie die u wilt installeren. Uninstall Docker Engine \u00b6 Uninstall the Docker Engine, CLI, Containerd, and Docker Compose packages: 1 sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-compose-plugin Images, containers, volumes, or customized configuration files on your host are not automatically removed. To delete all images, containers, and volumes: 1 2 sudo rm -rf /var/lib/docker sudo rm -rf /var/lib/containerd You must delete any edited configuration files manually. Volgende stap \u00b6 Continue to Post-installation steps for Linux . Review the topics in Develop with Docker to learn how to build new applications using Docker.","title":"Docker installeren"},{"location":"operating_systems/linux/ubuntu/docker/#docker-engine-installeren-op-ubuntu","text":"Om aan de slag te gaan met Docker Engine op Ubuntu, moet u ervoor zorgen dat u aan de vereisten voldoet en vervolgens Docker installeert .","title":"Docker Engine installeren op Ubuntu"},{"location":"operating_systems/linux/ubuntu/docker/#vereisten","text":"","title":"Vereisten"},{"location":"operating_systems/linux/ubuntu/docker/#os-vereisten","text":"Om Docker Engine te installeren, hebt u de 64-bits versie van een van deze Ubuntu-versies nodig: Ubuntu Jammy 22.04 (LTS) Ubuntu Impish 21.10 Ubuntu Focal 20.04 (LTS) Ubuntu Bionic 18.04 (LTS) Docker Engine wordt ondersteund op x86_64 (of amd64 ), armhf , arm64 , en s390x architecturen.","title":"OS-vereisten"},{"location":"operating_systems/linux/ubuntu/docker/#oude-versies-verwijderen","text":"Oudere versies van Docker werden docker , docker.io , of docker-engine . Als deze zijn ge\u00efnstalleerd, verwijder ze dan: 1 sudo apt-get remove docker docker-engine docker.io containerd runc Het is ok\u00e9 als apt-get wordt gemeld dat geen van deze pakketten is ge\u00efnstalleerd. De inhoud van /var/lib/docker/ , inclusief afbeeldingen, containers, volumes en netwerken, blijft behouden. Als u uw bestaande gegevens niet hoeft op te slaan en met een schone installatie wilt beginnen, raadpleegt u het gedeelte Docker Engine verwijderen onderaan deze pagina.","title":"Oude versies verwijderen"},{"location":"operating_systems/linux/ubuntu/docker/#installatiemethoden-_","text":"U kunt Docker Engine op verschillende manieren installeren, afhankelijk van uw behoeften: De meeste gebruikers stellen de repositories van Docker in en installeren van daaruit, voor eenvoudige installatie en upgradetaken. Dit is de aanbevolen aanpak. Sommige gebruikers downloaden het DEB-pakket en installeren het handmatig en beheren upgrades volledig handmatig. Dit is handig in situaties zoals het installeren van Docker op air-gapped systemen zonder toegang tot internet. In test- en ontwikkelomgevingen kiezen sommige gebruikers ervoor om geautomatiseerde gemaksscripts te gebruiken om Docker te installeren.","title":"Installatiemethoden _"},{"location":"operating_systems/linux/ubuntu/docker/#installeren-met-behulp-van-de-repository","text":"Voordat u Docker Engine voor de eerste keer op een nieuwe hostcomputer installeert, moet u de Docker-repository instellen. Daarna kunt u Docker installeren en bijwerken vanuit de repository.","title":"Installeren met behulp van de repository"},{"location":"operating_systems/linux/ubuntu/docker/#de-opslagplaats-instellen","text":"Werk de apt pakketindex bij en installeer pakketten om het apt gebruik van een repository via HTTPS toe te staan: 1 2 3 4 5 6 7 $ sudo apt-get update $ sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Voeg de offici\u00eble GPG-sleutel van Docker toe: 1 2 sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg Gebruik de volgende opdracht om de repository in te stellen: 1 2 3 $ echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null","title":"De opslagplaats instellen"},{"location":"operating_systems/linux/ubuntu/docker/#docker-engine-installeren","text":"Werk de apt pakketindex bij en installeer de nieuwste versie van Docker Engine, containerd en Docker Compose, of ga naar de volgende stap om een specifieke versie te installeren: 1 2 sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin Om een specifieke versie van Docker Engine te installeren, vermeldt u de beschikbare versies in de repo, selecteert u en installeert u: a. Maak een lijst van de beschikbare versies in uw repo: 1 2 3 4 5 6 $ apt-cache madison docker-ce docker-ce | 5 :20.10.16~3-0~ubuntu-jammy | https://download.docker.com/linux/ubuntu jammy/stable amd64 Packages docker-ce | 5 :20.10.15~3-0~ubuntu-jammy | https://download.docker.com/linux/ubuntu jammy/stable amd64 Packages docker-ce | 5 :20.10.14~3-0~ubuntu-jammy | https://download.docker.com/linux/ubuntu jammy/stable amd64 Packages docker-ce | 5 :20.10.13~3-0~ubuntu-jammy | https://download.docker.com/linux/ubuntu jammy/stable amd64 Packages b. Installeer een specifieke versie met behulp van de versiereeks uit de tweede kolom, bijvoorbeeld 5:20.10.16~3-0~ubuntu-jammy . 1 sudo apt-get install docker-ce = <VERSION_STRING> docker-ce-cli = <VERSION_STRING> containerd.io docker-compose-plugin Controleer of Docker Engine correct is ge\u00efnstalleerd door de hello-world afbeelding uit te voeren. 1 2 sudo service docker start sudo docker run hello-world Met deze opdracht wordt een testimage gedownload en uitgevoerd in een container. Wanneer de container wordt uitgevoerd, drukt deze een bericht af en wordt afgesloten. Docker Engine is ge\u00efnstalleerd en draait. De docker groep is gemaakt, maar er worden geen gebruikers aan toegevoegd. U moet gebruiken sudo om Docker-opdrachten uit te voeren. Ga door naar Linux na de installatie om niet-bevoegde gebruikers toe te staan Docker-opdrachten uit te voeren en voor andere optionele configuratiestappen.","title":"Docker Engine installeren"},{"location":"operating_systems/linux/ubuntu/docker/#docker-engine-upgraden","text":"Om Docker Engine te upgraden, voert u eerst uit sudo apt-get update , volgt u de installatie-instructies en kiest u de nieuwe versie die u wilt installeren.","title":"Docker-engine upgraden"},{"location":"operating_systems/linux/ubuntu/docker/#uninstall-docker-engine","text":"Uninstall the Docker Engine, CLI, Containerd, and Docker Compose packages: 1 sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-compose-plugin Images, containers, volumes, or customized configuration files on your host are not automatically removed. To delete all images, containers, and volumes: 1 2 sudo rm -rf /var/lib/docker sudo rm -rf /var/lib/containerd You must delete any edited configuration files manually.","title":"Uninstall Docker Engine"},{"location":"operating_systems/linux/ubuntu/docker/#volgende-stap","text":"Continue to Post-installation steps for Linux . Review the topics in Develop with Docker to learn how to build new applications using Docker.","title":"Volgende stap"},{"location":"operating_systems/linux/ubuntu/parallel/","tags":["linux","parallel","command"],"text":"Processing Linux Commands in Parallel \u00b6 1. Introduction \u00b6 There are many common tasks in Linux that we may want to consider running in parallel, such as: Downloading a large number of files Encoding/decoding a large number of images on a machine with multiple CPU cores Making a computation with many different parameters and storing the results Of course, we can accomplish all these tasks without using parallelization. But if we process each file, connection, or computation in several parallel processes, we can have a great advantage in terms of speed . Luckily, there are multiple powerful command-line tools for parallelization in Linux systems that can help us achieve this. In this tutorial, we\u2019re going to see how to use the Bash ampersand & operator, xargs , and GNU parallel to achieve parallelization on the Linux command line. 2. A Sample Task \u00b6 First, let\u2019s create a simple script that we\u2019ll run in parallel. Let\u2019s create a file named ./process with contents: 1 2 3 4 #!/bin/bash echo \"started processing $* ..\" sleep $(( 2 + RANDOM % 3 )) ; echo finished processing \" $* \" ; This script will fake an actual process that takes 2 to 5 seconds to complete. Let\u2019s make it executable to be able to use it: 1 chmod +x ./process 3. Using & \u00b6 As a basic way to run commands in parallel, we can use the built-in Bash ampersand & operator to run a command asynchronously so that the shell doesn\u2019t wait for the current command to complete before moving on to the next one: 1 2 ./process 1 & ./process 2 & This will create two processes that will start at essentially the same instant and run in parallel. Because we\u2019ve introduced random sleep times in our example script, the output may look like this: 1 2 3 4 5 6 7 8 9 [ 1 ] 25254 [ 2 ] 25255 started processing 1 .. started processing 2 .. finished processing 2 finished processing 1 [ 1 ] - Done ./process 1 [ 2 ] + Done ./process 2 Clearly, we can use this approach to run many parallel processes. But if we have many tasks \u2013 for example, a hundred images to be converted \u2013 we wouldn\u2019t want to start all hundred tasks at once, but instead, process them in batches to utilize our cores better. To achieve this, we need to wait for some tasks to complete before starting others. 3.1. Using wait with & \u00b6 The wait command will, by default, wait for all child processes to exit. So, using the wait command, we can run batches of operations: AD 1 2 3 4 5 6 7 8 9 10 11 echo \"starting batch 1..\" ./process 1 .jpg & ./process 2 .jpg & ./process 3 .jpg & wait echo \"starting batch 2..\" ./process 4 .jpg & ./process 5 .jpg & ./process 6 .jpg & wait echo \"finished\" However, there\u2019s one big downside to this approach. To utilize our CPU cores effectively, we\u2019d want a new process to start as soon as a running process ends. But with this solution, we wouldn\u2019t start new processes until all the tasks in the previous batch were completed. To overcome this limitation, we can use xargs . 4. Using _xargs \u00b6 xargs is a command-line tool that helps us run commands with arguments parsed from standard input. It can also parallelize our tasks for us. Let\u2019s try the previous input we used with &, but this time with xargs : 1 2 3 4 5 6 7 8 9 10 11 12 13 $ echo '1.jpg 2.jpg 3.jpg 4.jpg 5.jpg 6.jpg' | xargs -n 1 -P 3 ./process started processing 1 .jpg.. started processing 2 .jpg.. started processing 3 .jpg.. finished processing 1 .jpg finished processing 2 .jpg started processing 4 .jpg.. started processing 5 .jpg.. finished processing 3 .jpg started processing 6 .jpg.. finished processing 5 .jpg finished processing 4 .jpg finished processing 6 .jpg xargs immediately creates the next process once a process is completed. We specify the number of arguments per call using the -n argument and the number of parallel tasks using the -P argument. 4.1. Using Replacement \u00b6 If the executable we\u2019re using requires us to put the arguments to some specific place rather than appending them directly after the executable name, we can use replacement. Let\u2019s try it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ args = \"1\\n2\\n3\\n4\\n5\\n6\" $ echo -e $args | xargs -I \"{}\" -P 2 ./process {} .jpg started processing 1 .jpg.. started processing 2 .jpg.. finished processing 2 .jpg started processing 3 .jpg.. finished processing 1 .jpg started processing 4 .jpg.. finished processing 3 .jpg started processing 5 .jpg.. finished processing 5 .jpg started processing 6 .jpg.. finished processing 4 .jpg finished processing 6 .jpg 4.2. Handling Arguments With Newlines \u00b6 If the arguments we want to use with our processes include newline characters, we can use a null character (\\0) delimited input stream. For example, with the find command, we can set the output to be null-delimited instead of newline-delimited by using the -print0 flag: 1 find . -print0 | xargs -0 -n 2 -P 2 ./process As the arguments are now null-delimited, we can be sure that newline characters in the input will be preserved. 5. Using GNU parallel \u00b6 GNU parallel is one of the most advanced command-line tools available for running parallel tasks. It has many features, including the ability to distribute and run tasks remotely on multiple machines using ssh . 5.1. Basic Usage \u00b6 The basic usage of parallel is very similar to xargs . Actually, for simple cases, we can use it interchangeably with xargs . Let\u2019s try: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ args = \"1\\n2\\n3\\n4\\n5\\n6\" $ echo -e $args | parallel --ungroup --jobs 3 ./process started processing 1 .. started processing 2 .. started processing 3 .. finished processing 1 finished processing 2 finished processing 3 started processing 4 .. started processing 5 .. started processing 6 .. finished processing 5 finished processing 4 finished processing 6 The \u2013jobs argument is the same as the xargs command\u2019s -P argument, which determines the maximum number of parallel jobs to be running at the same time. By default, parallel will print the output of a process only after it is finished. The \u2013ungroup flag disables this functionality. We can use it to see the actual execution order of commands as they are running. We can supply the input arguments also via the command line. Let\u2019s try running it to get the same output as above: 1 parallel --ungroup --jobs 3 ./process ::: 1 2 3 4 5 6 when supplying command-line arguments, we can use ::: (three colons) to supply arguments directly, and :::: (four colons) to supply arguments from a file. Let\u2019s see an example that supplies input from a file: 1 2 3 args = \"1\\n2\\n3\\n4\\n5\\n6\" echo -e $args > input.txt parallel --ungroup --jobs 3 ./process :::: input.txt The output would be similar to the above. 5.2. Running Combinations of Multiple Sources \u00b6 We can use parallel to run tasks for every possible combination of two sources. Let\u2019s try it for two sample sources: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ parallel --ungroup ./process ::: 1 2 3 ::: 1 2 3 started processing 1 1 .. started processing 1 2 .. started processing 1 3 .. started processing 2 1 .. started processing 2 2 .. started processing 2 3 .. started processing 3 1 .. started processing 3 2 .. finished processing 1 1 finished processing 1 2 finished processing 2 1 finished processing 2 2 finished processing 3 1 started processing 3 3 .. finished processing 1 3 finished processing 3 2 finished processing 2 3 finished processing 3 3 5.3. Linking Sources \u00b6 If instead of running for every possible combination, we want to \u201clink\u201d them after one another, we would use the \u2013link flag. Let\u2019s try it with two different input sources: 1 2 3 4 5 6 7 $ parallel --link --ungroup ./process ::: 1 2 3 ::: 1 2 3 started processing 1 1 .. started processing 2 2 .. started processing 3 3 .. finished processing 1 1 finished processing 3 3 finished processing 2 2 5.4. Replacement Strings \u00b6 Like in xargs , we can use replacement strings in parallel . The default replacement string is {}. Let\u2019s try it with a prefix: 1 2 3 4 5 6 7 $ parallel --ungroup ./process item- {} ::: 1 2 3 started processing item-1.. started processing item-2.. started processing item-3.. finished processing item-1 finished processing item-2 finished processing item-3 Other replacement strings do different kinds of manipulations on the input. For example, {.} will remove the extension from the argument: 1 2 3 4 5 6 7 $ parallel --ungroup ./process { . } ::: 1 .jpg 2 .jpg 3 .jpg started processing 1 .. started processing 2 .. started processing 3 .. finished processing 2 finished processing 1 finished processing 3 If we want to use multiple different variables for each command, we can also do this using special replacement strings: 1 2 3 4 5 6 7 $ parallel --ungroup --link ./process { 1 } .jpg { 2 } .jpg { 3 } .jpg ::: 1 2 3 ::: 4 5 6 ::: 7 8 9 started processing 1 .jpg 4 .jpg 7 .jpg.. started processing 2 .jpg 5 .jpg 8 .jpg.. started processing 3 .jpg 6 .jpg 9 .jpg.. finished processing 1 .jpg 4 .jpg 7 .jpg finished processing 2 .jpg 5 .jpg 8 .jpg finished processing 3 .jpg 6 .jpg 9 .jpg There are also many more options for replacement strings that can be found in the parallel tutorial . 5.5. Reading Input From File Columns \u00b6 We can read the input from different columns of a text file. Let\u2019s try it with a tab-separated text file: 1 2 3 4 5 6 7 8 9 $ args = \"1\\t4\\n2\\t5\\n3\\t6\" $ echo -e $args > input_cols.txt $ parallel --colsep '\\t' --ungroup ./process [{ 1 }] [{ 2 }] :::: input_cols.txt started processing [ 1 ] [ 4 ] .. started processing [ 2 ] [ 5 ] .. started processing [ 3 ] [ 6 ] .. finished processing [ 2 ] [ 5 ] finished processing [ 3 ] [ 6 ] finished processing [ 1 ] [ 4 ] 5.6. Saving Output \u00b6 We can save the output of each process into a file by using the \u2013files flag: 1 2 3 4 $ parallel --files --link ./process ::: 1 2 3 ::: 1 2 3 /tmp/parnqFzp.par /tmp/parSv0nW.par /tmp/parGyNbz.par This will create *.par files with the output of our commands as the content. If we want to have a more friendly directory structure, we can use the \u2013results and \u2013header arguments to write the results to a folder in a hierarchy. Let\u2019s run a command to generate the directory tree: 1 parallel --results outdir --link ./process ::: a b c ::: d e f Now, let\u2019s check the output using the tree command: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 outdir \u2514\u2500\u2500 1 \u251c\u2500\u2500 a \u2502 \u2514\u2500\u2500 2 \u2502 \u2514\u2500\u2500 d \u2502 \u251c\u2500\u2500 seq \u2502 \u251c\u2500\u2500 stderr \u2502 \u2514\u2500\u2500 stdout \u251c\u2500\u2500 b \u2502 \u2514\u2500\u2500 2 \u2502 \u2514\u2500\u2500 e \u2502 \u251c\u2500\u2500 seq \u2502 \u251c\u2500\u2500 stderr \u2502 \u2514\u2500\u2500 stdout \u2514\u2500\u2500 c \u2514\u2500\u2500 2 \u2514\u2500\u2500 f \u251c\u2500\u2500 seq \u251c\u2500\u2500 stderr \u2514\u2500\u2500 stdout parallel generates the directory structure based on argument positions and values. 5.7. Progress Information \u00b6 We can also have parallel show an estimate of the remaining time based on current task runs: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ parallel --eta --colsep '\\t' --ungroup ./process [{ 1 }] [{ 2 }] :::: input_cols.txt started processing [ 1 ] [ 4 ] .. started processing [ 2 ] [ 5 ] .. Computers / CPU cores / Max jobs to run 1 :local / 8 / 3 started processing [ 3 ] [ 6 ] .. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete ETA: 0s Left: 3 AVG: 0 .00s local:3/0/100%/0.0s finished processing [ 1 ] [ 4 ] ETA: 0s Left: 2 AVG: 0 .00s local:2/1/100%/3.0s finished processing [ 2 ] [ 5 ] ETA: 0s Left: 2 AVG: 0 .00s local:2/1/100%/3.0s finished processing [ 3 ] [ 6 ] ETA: 0s Left: 1 AVG: 0 .00s local:1/2/100%/1.5s ETA: 0s Left: 0 AVG: 0 .00s local:0/3/100%/1.0s 5.8. Running Parallel Tasks on Remote Machines \u00b6 We can run our parallel tasks on remote machines using parallel through ssh . Let\u2019s assume we have access to host1 and host2 using our username and ssh keys that are added to our system. Let\u2019s try it: 1 2 3 4 5 $ parallel -S host1 -S host2 echo ::: running on remote hosts running on remote hosts The hosts that will run each command and the order of execution will change randomly with every run. 6. Conclusion[ \u00b6 In this article, we learned how to use the Bash ampersand & operator, xargs , and GNU parallel to parallelize our tasks on the command line.","title":"Parallel"},{"location":"operating_systems/linux/ubuntu/parallel/#processing-linux-commands-in-parallel","text":"","title":"Processing Linux Commands in Parallel"},{"location":"operating_systems/linux/ubuntu/parallel/#1-introduction","text":"There are many common tasks in Linux that we may want to consider running in parallel, such as: Downloading a large number of files Encoding/decoding a large number of images on a machine with multiple CPU cores Making a computation with many different parameters and storing the results Of course, we can accomplish all these tasks without using parallelization. But if we process each file, connection, or computation in several parallel processes, we can have a great advantage in terms of speed . Luckily, there are multiple powerful command-line tools for parallelization in Linux systems that can help us achieve this. In this tutorial, we\u2019re going to see how to use the Bash ampersand & operator, xargs , and GNU parallel to achieve parallelization on the Linux command line.","title":"1. Introduction"},{"location":"operating_systems/linux/ubuntu/parallel/#2-a-sample-task","text":"First, let\u2019s create a simple script that we\u2019ll run in parallel. Let\u2019s create a file named ./process with contents: 1 2 3 4 #!/bin/bash echo \"started processing $* ..\" sleep $(( 2 + RANDOM % 3 )) ; echo finished processing \" $* \" ; This script will fake an actual process that takes 2 to 5 seconds to complete. Let\u2019s make it executable to be able to use it: 1 chmod +x ./process","title":"2. A Sample Task"},{"location":"operating_systems/linux/ubuntu/parallel/#3-using","text":"As a basic way to run commands in parallel, we can use the built-in Bash ampersand & operator to run a command asynchronously so that the shell doesn\u2019t wait for the current command to complete before moving on to the next one: 1 2 ./process 1 & ./process 2 & This will create two processes that will start at essentially the same instant and run in parallel. Because we\u2019ve introduced random sleep times in our example script, the output may look like this: 1 2 3 4 5 6 7 8 9 [ 1 ] 25254 [ 2 ] 25255 started processing 1 .. started processing 2 .. finished processing 2 finished processing 1 [ 1 ] - Done ./process 1 [ 2 ] + Done ./process 2 Clearly, we can use this approach to run many parallel processes. But if we have many tasks \u2013 for example, a hundred images to be converted \u2013 we wouldn\u2019t want to start all hundred tasks at once, but instead, process them in batches to utilize our cores better. To achieve this, we need to wait for some tasks to complete before starting others.","title":"3. Using &amp;"},{"location":"operating_systems/linux/ubuntu/parallel/#31-using-wait-with","text":"The wait command will, by default, wait for all child processes to exit. So, using the wait command, we can run batches of operations: AD 1 2 3 4 5 6 7 8 9 10 11 echo \"starting batch 1..\" ./process 1 .jpg & ./process 2 .jpg & ./process 3 .jpg & wait echo \"starting batch 2..\" ./process 4 .jpg & ./process 5 .jpg & ./process 6 .jpg & wait echo \"finished\" However, there\u2019s one big downside to this approach. To utilize our CPU cores effectively, we\u2019d want a new process to start as soon as a running process ends. But with this solution, we wouldn\u2019t start new processes until all the tasks in the previous batch were completed. To overcome this limitation, we can use xargs .","title":"3.1. Using\u00a0wait\u00a0with &amp;"},{"location":"operating_systems/linux/ubuntu/parallel/#4-using-_xargs","text":"xargs is a command-line tool that helps us run commands with arguments parsed from standard input. It can also parallelize our tasks for us. Let\u2019s try the previous input we used with &, but this time with xargs : 1 2 3 4 5 6 7 8 9 10 11 12 13 $ echo '1.jpg 2.jpg 3.jpg 4.jpg 5.jpg 6.jpg' | xargs -n 1 -P 3 ./process started processing 1 .jpg.. started processing 2 .jpg.. started processing 3 .jpg.. finished processing 1 .jpg finished processing 2 .jpg started processing 4 .jpg.. started processing 5 .jpg.. finished processing 3 .jpg started processing 6 .jpg.. finished processing 5 .jpg finished processing 4 .jpg finished processing 6 .jpg xargs immediately creates the next process once a process is completed. We specify the number of arguments per call using the -n argument and the number of parallel tasks using the -P argument.","title":"4. Using\u00a0_xargs"},{"location":"operating_systems/linux/ubuntu/parallel/#41-using-replacement","text":"If the executable we\u2019re using requires us to put the arguments to some specific place rather than appending them directly after the executable name, we can use replacement. Let\u2019s try it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ args = \"1\\n2\\n3\\n4\\n5\\n6\" $ echo -e $args | xargs -I \"{}\" -P 2 ./process {} .jpg started processing 1 .jpg.. started processing 2 .jpg.. finished processing 2 .jpg started processing 3 .jpg.. finished processing 1 .jpg started processing 4 .jpg.. finished processing 3 .jpg started processing 5 .jpg.. finished processing 5 .jpg started processing 6 .jpg.. finished processing 4 .jpg finished processing 6 .jpg","title":"4.1. Using Replacement"},{"location":"operating_systems/linux/ubuntu/parallel/#42-handling-arguments-with-newlines","text":"If the arguments we want to use with our processes include newline characters, we can use a null character (\\0) delimited input stream. For example, with the find command, we can set the output to be null-delimited instead of newline-delimited by using the -print0 flag: 1 find . -print0 | xargs -0 -n 2 -P 2 ./process As the arguments are now null-delimited, we can be sure that newline characters in the input will be preserved.","title":"4.2. Handling Arguments With Newlines"},{"location":"operating_systems/linux/ubuntu/parallel/#5-using-gnu-parallel","text":"GNU parallel is one of the most advanced command-line tools available for running parallel tasks. It has many features, including the ability to distribute and run tasks remotely on multiple machines using ssh .","title":"5. Using GNU\u00a0parallel"},{"location":"operating_systems/linux/ubuntu/parallel/#51-basic-usage","text":"The basic usage of parallel is very similar to xargs . Actually, for simple cases, we can use it interchangeably with xargs . Let\u2019s try: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ args = \"1\\n2\\n3\\n4\\n5\\n6\" $ echo -e $args | parallel --ungroup --jobs 3 ./process started processing 1 .. started processing 2 .. started processing 3 .. finished processing 1 finished processing 2 finished processing 3 started processing 4 .. started processing 5 .. started processing 6 .. finished processing 5 finished processing 4 finished processing 6 The \u2013jobs argument is the same as the xargs command\u2019s -P argument, which determines the maximum number of parallel jobs to be running at the same time. By default, parallel will print the output of a process only after it is finished. The \u2013ungroup flag disables this functionality. We can use it to see the actual execution order of commands as they are running. We can supply the input arguments also via the command line. Let\u2019s try running it to get the same output as above: 1 parallel --ungroup --jobs 3 ./process ::: 1 2 3 4 5 6 when supplying command-line arguments, we can use ::: (three colons) to supply arguments directly, and :::: (four colons) to supply arguments from a file. Let\u2019s see an example that supplies input from a file: 1 2 3 args = \"1\\n2\\n3\\n4\\n5\\n6\" echo -e $args > input.txt parallel --ungroup --jobs 3 ./process :::: input.txt The output would be similar to the above.","title":"5.1. Basic Usage"},{"location":"operating_systems/linux/ubuntu/parallel/#52-running-combinations-of-multiple-sources","text":"We can use parallel to run tasks for every possible combination of two sources. Let\u2019s try it for two sample sources: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ parallel --ungroup ./process ::: 1 2 3 ::: 1 2 3 started processing 1 1 .. started processing 1 2 .. started processing 1 3 .. started processing 2 1 .. started processing 2 2 .. started processing 2 3 .. started processing 3 1 .. started processing 3 2 .. finished processing 1 1 finished processing 1 2 finished processing 2 1 finished processing 2 2 finished processing 3 1 started processing 3 3 .. finished processing 1 3 finished processing 3 2 finished processing 2 3 finished processing 3 3","title":"5.2. Running Combinations of Multiple Sources"},{"location":"operating_systems/linux/ubuntu/parallel/#53-linking-sources","text":"If instead of running for every possible combination, we want to \u201clink\u201d them after one another, we would use the \u2013link flag. Let\u2019s try it with two different input sources: 1 2 3 4 5 6 7 $ parallel --link --ungroup ./process ::: 1 2 3 ::: 1 2 3 started processing 1 1 .. started processing 2 2 .. started processing 3 3 .. finished processing 1 1 finished processing 3 3 finished processing 2 2","title":"5.3. Linking Sources"},{"location":"operating_systems/linux/ubuntu/parallel/#54-replacement-strings","text":"Like in xargs , we can use replacement strings in parallel . The default replacement string is {}. Let\u2019s try it with a prefix: 1 2 3 4 5 6 7 $ parallel --ungroup ./process item- {} ::: 1 2 3 started processing item-1.. started processing item-2.. started processing item-3.. finished processing item-1 finished processing item-2 finished processing item-3 Other replacement strings do different kinds of manipulations on the input. For example, {.} will remove the extension from the argument: 1 2 3 4 5 6 7 $ parallel --ungroup ./process { . } ::: 1 .jpg 2 .jpg 3 .jpg started processing 1 .. started processing 2 .. started processing 3 .. finished processing 2 finished processing 1 finished processing 3 If we want to use multiple different variables for each command, we can also do this using special replacement strings: 1 2 3 4 5 6 7 $ parallel --ungroup --link ./process { 1 } .jpg { 2 } .jpg { 3 } .jpg ::: 1 2 3 ::: 4 5 6 ::: 7 8 9 started processing 1 .jpg 4 .jpg 7 .jpg.. started processing 2 .jpg 5 .jpg 8 .jpg.. started processing 3 .jpg 6 .jpg 9 .jpg.. finished processing 1 .jpg 4 .jpg 7 .jpg finished processing 2 .jpg 5 .jpg 8 .jpg finished processing 3 .jpg 6 .jpg 9 .jpg There are also many more options for replacement strings that can be found in the parallel tutorial .","title":"5.4. Replacement Strings"},{"location":"operating_systems/linux/ubuntu/parallel/#55-reading-input-from-file-columns","text":"We can read the input from different columns of a text file. Let\u2019s try it with a tab-separated text file: 1 2 3 4 5 6 7 8 9 $ args = \"1\\t4\\n2\\t5\\n3\\t6\" $ echo -e $args > input_cols.txt $ parallel --colsep '\\t' --ungroup ./process [{ 1 }] [{ 2 }] :::: input_cols.txt started processing [ 1 ] [ 4 ] .. started processing [ 2 ] [ 5 ] .. started processing [ 3 ] [ 6 ] .. finished processing [ 2 ] [ 5 ] finished processing [ 3 ] [ 6 ] finished processing [ 1 ] [ 4 ]","title":"5.5. Reading Input From File Columns"},{"location":"operating_systems/linux/ubuntu/parallel/#56-saving-output","text":"We can save the output of each process into a file by using the \u2013files flag: 1 2 3 4 $ parallel --files --link ./process ::: 1 2 3 ::: 1 2 3 /tmp/parnqFzp.par /tmp/parSv0nW.par /tmp/parGyNbz.par This will create *.par files with the output of our commands as the content. If we want to have a more friendly directory structure, we can use the \u2013results and \u2013header arguments to write the results to a folder in a hierarchy. Let\u2019s run a command to generate the directory tree: 1 parallel --results outdir --link ./process ::: a b c ::: d e f Now, let\u2019s check the output using the tree command: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 outdir \u2514\u2500\u2500 1 \u251c\u2500\u2500 a \u2502 \u2514\u2500\u2500 2 \u2502 \u2514\u2500\u2500 d \u2502 \u251c\u2500\u2500 seq \u2502 \u251c\u2500\u2500 stderr \u2502 \u2514\u2500\u2500 stdout \u251c\u2500\u2500 b \u2502 \u2514\u2500\u2500 2 \u2502 \u2514\u2500\u2500 e \u2502 \u251c\u2500\u2500 seq \u2502 \u251c\u2500\u2500 stderr \u2502 \u2514\u2500\u2500 stdout \u2514\u2500\u2500 c \u2514\u2500\u2500 2 \u2514\u2500\u2500 f \u251c\u2500\u2500 seq \u251c\u2500\u2500 stderr \u2514\u2500\u2500 stdout parallel generates the directory structure based on argument positions and values.","title":"5.6. Saving Output"},{"location":"operating_systems/linux/ubuntu/parallel/#57-progress-information","text":"We can also have parallel show an estimate of the remaining time based on current task runs: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ parallel --eta --colsep '\\t' --ungroup ./process [{ 1 }] [{ 2 }] :::: input_cols.txt started processing [ 1 ] [ 4 ] .. started processing [ 2 ] [ 5 ] .. Computers / CPU cores / Max jobs to run 1 :local / 8 / 3 started processing [ 3 ] [ 6 ] .. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete ETA: 0s Left: 3 AVG: 0 .00s local:3/0/100%/0.0s finished processing [ 1 ] [ 4 ] ETA: 0s Left: 2 AVG: 0 .00s local:2/1/100%/3.0s finished processing [ 2 ] [ 5 ] ETA: 0s Left: 2 AVG: 0 .00s local:2/1/100%/3.0s finished processing [ 3 ] [ 6 ] ETA: 0s Left: 1 AVG: 0 .00s local:1/2/100%/1.5s ETA: 0s Left: 0 AVG: 0 .00s local:0/3/100%/1.0s","title":"5.7. Progress Information"},{"location":"operating_systems/linux/ubuntu/parallel/#58-running-parallel-tasks-on-remote-machines","text":"We can run our parallel tasks on remote machines using parallel through ssh . Let\u2019s assume we have access to host1 and host2 using our username and ssh keys that are added to our system. Let\u2019s try it: 1 2 3 4 5 $ parallel -S host1 -S host2 echo ::: running on remote hosts running on remote hosts The hosts that will run each command and the order of execution will change randomly with every run.","title":"5.8. Running Parallel Tasks on Remote Machines"},{"location":"operating_systems/linux/ubuntu/parallel/#6-conclusion","text":"In this article, we learned how to use the Bash ampersand & operator, xargs , and GNU parallel to parallelize our tasks on the command line.","title":"6. Conclusion["},{"location":"operating_systems/linux/ubuntu/snippets/","text":"Ubuntu - Snippets \u00b6 Beschrijving \u00b6 Deze snippets werken in Ubuntu of andere debian -gebaseerde operating systems . Show last 5 modified files \u00b6 1 ls -lht | head -6 where: -l outputs in a list format -h makes output human readable (i.e. file sizes appear in kb, mb, etc.) -t sorts output by placing most recently modified file first head -6 will show 5 files because ls prints the block size in the first line of output. I think this is a slightly more elegant and possibly more useful approach. Example output: 1 total 26960312 -rw-r--r--@ 1 user staff 1 .2K 11 Jan 11 :22 phone2.7.py -rw-r--r--@ 1 user staff 2 .7M 10 Jan 15 :26 03 -cookies-1.pdf -rw-r--r--@ 1 user staff 9 .2M 9 Jan 16 :21 Wk1_sem.pdf -rw-r--r--@ 1 user staff 502K 8 Jan 10 :20 lab-01.pdf -rw-rw-rw-@ 1 user staff 2 .0M 5 Jan 22 :06 0410 -1.wmv Set static IP \u00b6 1 sudo nano /etc/netplan/00-installer-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 # This file describes the network interfaces available on your system # For more information, see netplan(5). network : version : 2 renderer : networkd ethernets : ens33 : #pas dit aan dhcp4 : no #yes/no dhcp6 : no #yes/no addresses : [ 192.168.1.100/24 ] #pas dit aan gateway4 : 192.168.1.1 #pas dit aan nameservers : addresses : [ 8.8.8.8 , 8.8.4.4 ] 1 sudo netplan apply Upgrade Ubuntu 20.04 to 22.04 \u00b6 1 2 sudo apt update & sudo apt upgrade -y sudo do -release-upgrade Sluit een process op een gegeven poort \u00b6 1 sudo kill -9 ` sudo lsof -t -i:9001 `","title":"Ubuntu Snippets"},{"location":"operating_systems/linux/ubuntu/snippets/#ubuntu-snippets","text":"","title":"Ubuntu - Snippets"},{"location":"operating_systems/linux/ubuntu/snippets/#beschrijving","text":"Deze snippets werken in Ubuntu of andere debian -gebaseerde operating systems .","title":"Beschrijving"},{"location":"operating_systems/linux/ubuntu/snippets/#show-last-5-modified-files","text":"1 ls -lht | head -6 where: -l outputs in a list format -h makes output human readable (i.e. file sizes appear in kb, mb, etc.) -t sorts output by placing most recently modified file first head -6 will show 5 files because ls prints the block size in the first line of output. I think this is a slightly more elegant and possibly more useful approach. Example output: 1 total 26960312 -rw-r--r--@ 1 user staff 1 .2K 11 Jan 11 :22 phone2.7.py -rw-r--r--@ 1 user staff 2 .7M 10 Jan 15 :26 03 -cookies-1.pdf -rw-r--r--@ 1 user staff 9 .2M 9 Jan 16 :21 Wk1_sem.pdf -rw-r--r--@ 1 user staff 502K 8 Jan 10 :20 lab-01.pdf -rw-rw-rw-@ 1 user staff 2 .0M 5 Jan 22 :06 0410 -1.wmv","title":"Show last 5 modified files"},{"location":"operating_systems/linux/ubuntu/snippets/#set-static-ip","text":"1 sudo nano /etc/netplan/00-installer-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 # This file describes the network interfaces available on your system # For more information, see netplan(5). network : version : 2 renderer : networkd ethernets : ens33 : #pas dit aan dhcp4 : no #yes/no dhcp6 : no #yes/no addresses : [ 192.168.1.100/24 ] #pas dit aan gateway4 : 192.168.1.1 #pas dit aan nameservers : addresses : [ 8.8.8.8 , 8.8.4.4 ] 1 sudo netplan apply","title":"Set static IP"},{"location":"operating_systems/linux/ubuntu/snippets/#upgrade-ubuntu-2004-to-2204","text":"1 2 sudo apt update & sudo apt upgrade -y sudo do -release-upgrade","title":"Upgrade Ubuntu 20.04 to 22.04"},{"location":"operating_systems/linux/ubuntu/snippets/#sluit-een-process-op-een-gegeven-poort","text":"1 sudo kill -9 ` sudo lsof -t -i:9001 `","title":"Sluit een process op een gegeven poort"},{"location":"operating_systems/linux/ubuntu/ubuntu/","text":"Ubuntu \u00b6","title":"Ubuntu"},{"location":"operating_systems/linux/ubuntu/ubuntu/#ubuntu","text":"","title":"Ubuntu"},{"location":"programming/programming/","text":"","title":"Programming"},{"location":"programming/devops/github/github/","text":"Github \u00b6","title":"Github"},{"location":"programming/devops/github/github/#github","text":"","title":"Github"},{"location":"programming/devops/github/github_commands/","text":"Handige commando's \u00b6 Multiple Git pull \u00b6 Indien je in meerdere onderliggende folders git repo's hebt zitten, kan je met dit commando alle git repos laten pullen. Dankzij parallel zal dit niet tegelijkertijd uitgevoerd worden, maar op meerdere tasks in parallel. 1 find . -maxdepth 8 -name '.git' -prune -type d -printf '%h\\n' | parallel --eta 'echo {} && git -C {} pull' Git alias \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function gc () { git commit -m \" $* \" } alias ga = 'git add' alias gp = 'git push' alias gl = 'git log' alias gs = 'git status' alias gd = 'git diff' alias gdc = 'git diff --cached' alias gb = 'git branch' alias gra = 'git remote add' alias grr = 'git remote rm' alias gpu = 'git pull' alias gcl = 'git clone' Submodules \u00b6 Git clone \u00b6 1 git clone --recurse-submodules git@github.com:havenstadrp/resources Git pull alle subfolders \u00b6 1 find . -maxdepth 8 -name '.git' -prune -type d -printf '%h\\n' | parallel --eta 'echo {} && git -C {} pull' Config \u00b6 File permissions \u00b6 1 git config core.filemode false User \"login\" \u00b6 1 2 git config --global user.email \"email@mail.com\" git config --global user.name \"username\"","title":"Github Commands"},{"location":"programming/devops/github/github_commands/#handige-commandos","text":"","title":"Handige commando's"},{"location":"programming/devops/github/github_commands/#multiple-git-pull","text":"Indien je in meerdere onderliggende folders git repo's hebt zitten, kan je met dit commando alle git repos laten pullen. Dankzij parallel zal dit niet tegelijkertijd uitgevoerd worden, maar op meerdere tasks in parallel. 1 find . -maxdepth 8 -name '.git' -prune -type d -printf '%h\\n' | parallel --eta 'echo {} && git -C {} pull'","title":"Multiple Git pull"},{"location":"programming/devops/github/github_commands/#git-alias","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function gc () { git commit -m \" $* \" } alias ga = 'git add' alias gp = 'git push' alias gl = 'git log' alias gs = 'git status' alias gd = 'git diff' alias gdc = 'git diff --cached' alias gb = 'git branch' alias gra = 'git remote add' alias grr = 'git remote rm' alias gpu = 'git pull' alias gcl = 'git clone'","title":"Git alias"},{"location":"programming/devops/github/github_commands/#submodules","text":"","title":"Submodules"},{"location":"programming/devops/github/github_commands/#git-clone","text":"1 git clone --recurse-submodules git@github.com:havenstadrp/resources","title":"Git clone"},{"location":"programming/devops/github/github_commands/#git-pull-alle-subfolders","text":"1 find . -maxdepth 8 -name '.git' -prune -type d -printf '%h\\n' | parallel --eta 'echo {} && git -C {} pull'","title":"Git pull alle subfolders"},{"location":"programming/devops/github/github_commands/#config","text":"","title":"Config"},{"location":"programming/devops/github/github_commands/#file-permissions","text":"1 git config core.filemode false","title":"File permissions"},{"location":"programming/devops/github/github_commands/#user-login","text":"1 2 git config --global user.email \"email@mail.com\" git config --global user.name \"username\"","title":"User \"login\""},{"location":"programming/python/python/","text":"","title":"Python"},{"location":"programming/yaml/yaml/","text":"","title":"YAML"},{"location":"security/vaultwarden/","text":"","title":"Vaultwarden"},{"location":"server/automation/ansible/ansible/","text":"Ansible \u00b6 Installing and upgrading Ansible \u00b6 Locating Python \u00b6 Locate and remember the path to the Python interpreter you wish to use to run Ansible. The following instructions refer to this Python as python3 . For example, if you\u2019ve determined that you want the Python at /usr/bin/python3.9 to be the one that you\u2019ll install Ansible under, specify that instead of python3 . Ensuring pip is available \u00b6 To verify whether pip is already installed for your preferred Python : 1 python3 -m pip -V If all is well, you should see something like the following: 1 2 3 $ python3 -m pip -V pip 21 .0.1 from /usr/lib/python3.9/site-packages/pip ( python 3 .9 ) If so, pip is available, and you can move on to the next step If you see an error like No module named pip , you\u2019ll need to install pip under your chosen Python interpreter before proceeding. This may mean installing an additional OS package (for example, python3-pip ), or installing the latest pip directly from the Python Packaging Authority by running the following: 1 2 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py --user You may need to perform some additional configuration before you are able to run Ansible. See the Python documentation on installing to the user site for more information. Installing Ansible \u00b6 Use pip in your selected Python environment to install the Ansible package of your choice for the current user: 1 python3 -m pip install --user ansible Alternately, you can install a specific version of ansible-core in this Python environment: 1 python3 -m pip install --user ansible-core == 2 .12.3 Upgrading Ansible \u00b6 To upgrade an existing Ansible installation in this Python environment to the latest released version, simply add --upgrade to the command above: 1 python3 -m pip install --upgrade --user ansible Confirming your installation \u00b6 You can test that Ansible is installed correctly by checking the version: 1 ansible --version The version displayed by this command is for the associated ansible-core package that has been installed. To check the version of the ansible package that has been installed: 1 python3 -m pip show ansible","title":"Ansible"},{"location":"server/automation/ansible/ansible/#ansible","text":"","title":"Ansible"},{"location":"server/automation/ansible/ansible/#installing-and-upgrading-ansible","text":"","title":"Installing and upgrading Ansible"},{"location":"server/automation/ansible/ansible/#locating-python","text":"Locate and remember the path to the Python interpreter you wish to use to run Ansible. The following instructions refer to this Python as python3 . For example, if you\u2019ve determined that you want the Python at /usr/bin/python3.9 to be the one that you\u2019ll install Ansible under, specify that instead of python3 .","title":"Locating Python"},{"location":"server/automation/ansible/ansible/#ensuring-pip-is-available","text":"To verify whether pip is already installed for your preferred Python : 1 python3 -m pip -V If all is well, you should see something like the following: 1 2 3 $ python3 -m pip -V pip 21 .0.1 from /usr/lib/python3.9/site-packages/pip ( python 3 .9 ) If so, pip is available, and you can move on to the next step If you see an error like No module named pip , you\u2019ll need to install pip under your chosen Python interpreter before proceeding. This may mean installing an additional OS package (for example, python3-pip ), or installing the latest pip directly from the Python Packaging Authority by running the following: 1 2 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py --user You may need to perform some additional configuration before you are able to run Ansible. See the Python documentation on installing to the user site for more information.","title":"Ensuring\u00a0pip\u00a0is available"},{"location":"server/automation/ansible/ansible/#installing-ansible","text":"Use pip in your selected Python environment to install the Ansible package of your choice for the current user: 1 python3 -m pip install --user ansible Alternately, you can install a specific version of ansible-core in this Python environment: 1 python3 -m pip install --user ansible-core == 2 .12.3","title":"Installing Ansible"},{"location":"server/automation/ansible/ansible/#upgrading-ansible","text":"To upgrade an existing Ansible installation in this Python environment to the latest released version, simply add --upgrade to the command above: 1 python3 -m pip install --upgrade --user ansible","title":"Upgrading Ansible"},{"location":"server/automation/ansible/ansible/#confirming-your-installation","text":"You can test that Ansible is installed correctly by checking the version: 1 ansible --version The version displayed by this command is for the associated ansible-core package that has been installed. To check the version of the ansible package that has been installed: 1 python3 -m pip show ansible","title":"Confirming your installation"},{"location":"server/automation/terraform/terraform/","text":"","title":"Terraform"},{"location":"server/management/ssh/ssh/","text":"SSH \u00b6","title":"SSH"},{"location":"server/management/ssh/ssh/#ssh","text":"","title":"SSH"},{"location":"server/proxmox/snippets/","text":"Proxmox - Snippets \u00b6 Disk Management \u00b6 Ubuntu VM Disk vergroten in Proxmox \u00b6 Als je in Proxmox een disk vergroot, zal deze door de OS nog niet volledig gebruikt kunnen worden. Daarom moeten we, na de vergroting in proxmox, deze alsnog met GParted en via de CLI resizen zodat de volledige grootte herkend kan worden door het Operating System . 1. Virtuele machine uitzetten \u00b6 Schakel de virtuele machine eerst helemaal uit. 2. Harde schijf resizen \u00b6 Nu kan je de harde schijf vergroten. Proxmox > Hardware Settings 3. GParted downloaden en opstarten \u00b6 Download laatste Gparted ISO van de GParted download website Importeer de GParted ISO file in Proxmox Start de VM en laat deze starten vanaf de ISO. (Duw snel op Esc ) 4. Partitie resizen met GParted \u00b6 Zoek de correcte partitie Resize de partitie ( Right Button -> resize) Duw opt vingske \ud83d\ude42 5. CLI resizing \u00b6 Check de huidige disksize met commando: 1 df -h Laat Ubuntu de volledige disk gebruiken: 1 sudo /sbin/lvresize -l +100%FREE /dev/mapper/ubuntu--vg-ubuntu--lv Gebruik resize2fs op deze disk 1 sudo /sbin/resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv Hierna kan je nogmaals de disksize bekijken. 1 df -h Als er geen fouten opgetreden zijn, heb je nu net de harde schijf vergroot!","title":"Snippets"},{"location":"server/proxmox/snippets/#proxmox-snippets","text":"","title":"Proxmox - Snippets"},{"location":"server/proxmox/snippets/#disk-management","text":"","title":"Disk Management"},{"location":"server/proxmox/snippets/#ubuntu-vm-disk-vergroten-in-proxmox","text":"Als je in Proxmox een disk vergroot, zal deze door de OS nog niet volledig gebruikt kunnen worden. Daarom moeten we, na de vergroting in proxmox, deze alsnog met GParted en via de CLI resizen zodat de volledige grootte herkend kan worden door het Operating System .","title":"Ubuntu VM Disk vergroten in Proxmox"},{"location":"server/proxmox/snippets/#1-virtuele-machine-uitzetten","text":"Schakel de virtuele machine eerst helemaal uit.","title":"1. Virtuele machine uitzetten"},{"location":"server/proxmox/snippets/#2-harde-schijf-resizen","text":"Nu kan je de harde schijf vergroten. Proxmox > Hardware Settings","title":"2. Harde schijf resizen"},{"location":"server/proxmox/snippets/#3-gparted-downloaden-en-opstarten","text":"Download laatste Gparted ISO van de GParted download website Importeer de GParted ISO file in Proxmox Start de VM en laat deze starten vanaf de ISO. (Duw snel op Esc )","title":"3. GParted downloaden en opstarten"},{"location":"server/proxmox/snippets/#4-partitie-resizen-met-gparted","text":"Zoek de correcte partitie Resize de partitie ( Right Button -> resize) Duw opt vingske \ud83d\ude42","title":"4. Partitie resizen met GParted"},{"location":"server/proxmox/snippets/#5-cli-resizing","text":"Check de huidige disksize met commando: 1 df -h Laat Ubuntu de volledige disk gebruiken: 1 sudo /sbin/lvresize -l +100%FREE /dev/mapper/ubuntu--vg-ubuntu--lv Gebruik resize2fs op deze disk 1 sudo /sbin/resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv Hierna kan je nogmaals de disksize bekijken. 1 df -h Als er geen fouten opgetreden zijn, heb je nu net de harde schijf vergroot!","title":"5. CLI resizing"},{"location":"server/raspberry_pi/raspberry_pi/","text":"Raspberry Pi \u00b6 Wat is een Raspberry Pi? \u00b6 Een Raspberry Pi is een minicomputer. De minicomputer is ongeveer zo net zo groot als een creditcard en past makkelijk in je broekzak. Het is een printplaatje en bevat alle belangrijke componenten die een computer nodig heeft, zoals de ARM-processor en alle benodigde aansluitingen. Stel dat jij een monitor, muis en een toetsenbord zou aansluiten op de Raspberry Pi, dan heb je gewoon een echte computer. Zoals we net al zeiden, bevat de Raspberry Pi veel aansluitingen die een \u201cgewone\u201d computer ook heeft, zoals USB, UTP poort (Internetpoort), HDMI en micro USB (Oplaadpoort/voeding). De Raspberry Pi heeft geen opslag en daarom heb je wel een micro-SD kaartje nodig, anders kan je er weinig mee. Het micro-SD kaartje kan je makkelijk in de micro-SD slot/gleuf doen. Op dit micro-SD kaartje zet je bijvoorbeeld software die je zelf hebt geprogrammeerd. Hiermee stuur je bijvoorbeeld motortjes aan of je verwerkt data van sensoren. Het is bijvoorbeeld ook mogelijk om een besturingssysteem erop te zetten, dat is wat makkelijker werken als je het aansluit op een monitor. Misschien kan je je nu al een beetje indenken dat er heel veel mogelijkheden zijn met een Raspberry Pi, maar daar komen we later in dit artikel op terug. Het is ook wel leuk om even het verleden in te duiken. Aan de universiteit van Cambridge is de Raspberry Pi ontwikkeld. Het was vooral bedoeld om kinderen enthousiast te maken om te gaan programmeren. De Raspberry Pi is heel goedkoop vergeleken met een \u201cechte\u201d computer en daarom werd het nog toegankelijker. Door de goedkope prijs was ook het doel om mensen in derdewereldlanden meer kennis te laten maken met een (mini)computer. In 2012 kwam de eerste Raspberry Pi Model B op de markt. Models \u00b6 Raspberry Pi Platform CPU RAM I/O Ports Price Raspberry Pi 400 1.8 Hz, Quad-core Broadcom BCM2711 (Cortex-A72) 4GB (LPDDR4) 2 \u00d7 USB 3.0, 1 x USB 2.0 ports, 2 x micro HDMI, 1 x Gigabit Ethernet $70 Raspberry Pi 4B 1.8 Hz, Quad-core Broadcom BCM2711 (Cortex-A72) 8GB (LPDDR4) 2x USB 3.0, 2x USB 2.0, 1x Gigabit Ethernet, 2x micro HDMI $75 Raspberry Pi 4B 1.8 Hz, Quad-core Broadcom BCM2711 (Cortex-A72) 4GB (LPDDR4) 2x USB 3.0, 2x USB 2.0, 1x Gigabit Ethernet, 2x micro HDMI $55 Raspberry Pi 4B 1.8 Hz, Quad-core Broadcom BCM2711 (Cortex-A72) 2GB (LPDDR4) 2x USB 3.0, 2x USB 2.0, 1x Gigabit Ethernet, 2x micro HDMI $35 Raspberry Pi 3B+ 1.4-GHz, 4-core Broadcom BCM2837B0 (Cortex-A53) 1GB 4 x USB 2.0, HDMI, 3.5mm audio $35 Raspberry Pi Zero WH 1-GHz, 1-core Broadcom BCM2835 (ARM1176JZF-S) 512MB 1x micro USB, 1x mini HDMI $17 Raspberry Pi Zero W 1-GHz, 1-core Broadcom BCM2835 (ARM1176JZF-S) 512MB 1x micro USB, 1x mini HDMI $10 Raspberry Pi Zero 1-GHz, 1-core Broadcom BCM2835 (ARM1176JZF-S) 512MB 1x micro USB, 1x mini HDMI $5 Raspberry Pi OS \u00b6 Raspberry Pi OS is a Debian-based Operating systems for Raspberry Pi. Since 2013, it has been officially provided by the Raspberry Pi Foundation as the primary operating system for the Raspberry Pi family of compact single-board computers. SSD aan een raspbarry pi hangen \u00b6 In cmdline.txt moet je deze tekst plakken op het einde van de eerste regel: 1 usb-storage.quirks = 152d:0578:u","title":"Raspberry Pi"},{"location":"server/raspberry_pi/raspberry_pi/#raspberry-pi","text":"","title":"Raspberry Pi"},{"location":"server/raspberry_pi/raspberry_pi/#wat-is-een-raspberry-pi","text":"Een Raspberry Pi is een minicomputer. De minicomputer is ongeveer zo net zo groot als een creditcard en past makkelijk in je broekzak. Het is een printplaatje en bevat alle belangrijke componenten die een computer nodig heeft, zoals de ARM-processor en alle benodigde aansluitingen. Stel dat jij een monitor, muis en een toetsenbord zou aansluiten op de Raspberry Pi, dan heb je gewoon een echte computer. Zoals we net al zeiden, bevat de Raspberry Pi veel aansluitingen die een \u201cgewone\u201d computer ook heeft, zoals USB, UTP poort (Internetpoort), HDMI en micro USB (Oplaadpoort/voeding). De Raspberry Pi heeft geen opslag en daarom heb je wel een micro-SD kaartje nodig, anders kan je er weinig mee. Het micro-SD kaartje kan je makkelijk in de micro-SD slot/gleuf doen. Op dit micro-SD kaartje zet je bijvoorbeeld software die je zelf hebt geprogrammeerd. Hiermee stuur je bijvoorbeeld motortjes aan of je verwerkt data van sensoren. Het is bijvoorbeeld ook mogelijk om een besturingssysteem erop te zetten, dat is wat makkelijker werken als je het aansluit op een monitor. Misschien kan je je nu al een beetje indenken dat er heel veel mogelijkheden zijn met een Raspberry Pi, maar daar komen we later in dit artikel op terug. Het is ook wel leuk om even het verleden in te duiken. Aan de universiteit van Cambridge is de Raspberry Pi ontwikkeld. Het was vooral bedoeld om kinderen enthousiast te maken om te gaan programmeren. De Raspberry Pi is heel goedkoop vergeleken met een \u201cechte\u201d computer en daarom werd het nog toegankelijker. Door de goedkope prijs was ook het doel om mensen in derdewereldlanden meer kennis te laten maken met een (mini)computer. In 2012 kwam de eerste Raspberry Pi Model B op de markt.","title":"Wat is een Raspberry Pi?"},{"location":"server/raspberry_pi/raspberry_pi/#models","text":"Raspberry Pi Platform CPU RAM I/O Ports Price Raspberry Pi 400 1.8 Hz, Quad-core Broadcom BCM2711 (Cortex-A72) 4GB (LPDDR4) 2 \u00d7 USB 3.0, 1 x USB 2.0 ports, 2 x micro HDMI, 1 x Gigabit Ethernet $70 Raspberry Pi 4B 1.8 Hz, Quad-core Broadcom BCM2711 (Cortex-A72) 8GB (LPDDR4) 2x USB 3.0, 2x USB 2.0, 1x Gigabit Ethernet, 2x micro HDMI $75 Raspberry Pi 4B 1.8 Hz, Quad-core Broadcom BCM2711 (Cortex-A72) 4GB (LPDDR4) 2x USB 3.0, 2x USB 2.0, 1x Gigabit Ethernet, 2x micro HDMI $55 Raspberry Pi 4B 1.8 Hz, Quad-core Broadcom BCM2711 (Cortex-A72) 2GB (LPDDR4) 2x USB 3.0, 2x USB 2.0, 1x Gigabit Ethernet, 2x micro HDMI $35 Raspberry Pi 3B+ 1.4-GHz, 4-core Broadcom BCM2837B0 (Cortex-A53) 1GB 4 x USB 2.0, HDMI, 3.5mm audio $35 Raspberry Pi Zero WH 1-GHz, 1-core Broadcom BCM2835 (ARM1176JZF-S) 512MB 1x micro USB, 1x mini HDMI $17 Raspberry Pi Zero W 1-GHz, 1-core Broadcom BCM2835 (ARM1176JZF-S) 512MB 1x micro USB, 1x mini HDMI $10 Raspberry Pi Zero 1-GHz, 1-core Broadcom BCM2835 (ARM1176JZF-S) 512MB 1x micro USB, 1x mini HDMI $5","title":"Models"},{"location":"server/raspberry_pi/raspberry_pi/#raspberry-pi-os","text":"Raspberry Pi OS is a Debian-based Operating systems for Raspberry Pi. Since 2013, it has been officially provided by the Raspberry Pi Foundation as the primary operating system for the Raspberry Pi family of compact single-board computers.","title":"Raspberry Pi OS"},{"location":"server/raspberry_pi/raspberry_pi/#ssd-aan-een-raspbarry-pi-hangen","text":"In cmdline.txt moet je deze tekst plakken op het einde van de eerste regel: 1 usb-storage.quirks = 152d:0578:u","title":"SSD aan een raspbarry pi hangen"},{"location":"server/truenas/truenas/","text":"","title":"TrueNAS"},{"location":"server/unraid/unraid/","text":"","title":"Unraid"},{"location":"software/discord/discord/","text":"","title":"Discord"},{"location":"software/tools/balena-etcher/","text":"Balena Etcher \u00b6 Wat is Balena Etcher? \u00b6 balenaEtcher is a free and open-source utility used for writing image files such as .iso and .img files, as well as zipped folders onto storage media to create live SD cards and USB flash drives. Links \u00b6 Website","title":"Balena Etcher"},{"location":"software/tools/balena-etcher/#balena-etcher","text":"","title":"Balena Etcher"},{"location":"software/tools/balena-etcher/#wat-is-balena-etcher","text":"balenaEtcher is a free and open-source utility used for writing image files such as .iso and .img files, as well as zipped folders onto storage media to create live SD cards and USB flash drives.","title":"Wat is Balena Etcher?"},{"location":"software/tools/balena-etcher/#links","text":"Website","title":"Links"},{"location":"software/tools/rpi-imager/","text":"Raspberry Pi Imager \u00b6 Wat is RPI Imager? \u00b6 Raspberry Pi Imager is the quick and easy way to install Raspberry Pi OS and other operating systems to a microSD card, ready to use with your Raspberry Pi. Watch our 45-second video to learn how to install an operating system using Raspberry Pi Imager. Download and install Raspberry Pi Imager to a computer with an SD card reader. Put the SD card you'll use with your Raspberry Pi into the reader and run Raspberry Pi Imager. Install on Raspberry Pi OS \u00b6 To install on Raspberry Pi OS , type sudo apt install rpi-imager in a Terminal window.","title":"RPI Imager"},{"location":"software/tools/rpi-imager/#raspberry-pi-imager","text":"","title":"Raspberry Pi Imager"},{"location":"software/tools/rpi-imager/#wat-is-rpi-imager","text":"Raspberry Pi Imager is the quick and easy way to install Raspberry Pi OS and other operating systems to a microSD card, ready to use with your Raspberry Pi. Watch our 45-second video to learn how to install an operating system using Raspberry Pi Imager. Download and install Raspberry Pi Imager to a computer with an SD card reader. Put the SD card you'll use with your Raspberry Pi into the reader and run Raspberry Pi Imager.","title":"Wat is RPI Imager?"},{"location":"software/tools/rpi-imager/#install-on-raspberry-pi-os","text":"To install on Raspberry Pi OS , type sudo apt install rpi-imager in a Terminal window.","title":"Install on Raspberry Pi OS"},{"location":"software/tools/rufus/","text":"Rufus \u00b6 Wat is Rufus? \u00b6 (https://rufus.ie/pics/rufus_en.png) Rufus is a utility that helps format and create bootable USB flash drives, such as USB keys/pendrives, memory sticks, etc. It can be especially useful for cases where: you need to create USB installation media from bootable ISOs (Windows, Linux, UEFI, etc.) you need to work on a system that doesn't have an OS installed you need to flash a BIOS or other firmware from DOS you want to run a low-level utility Despite its small size, Rufus provides everything you need! Oh, and Rufus is fast . For instance it's about twice as fast as UNetbootin , Universal USB Installer or Windows 7 USB download tool , on the creation of a Windows 7 USB installation drive from an ISO. It is also marginally faster on the creation of Linux bootable USB from ISOs. A non exhaustive list of Rufus supported ISOs is also provided at the bottom of this page.","title":"Rufus"},{"location":"software/tools/rufus/#rufus","text":"","title":"Rufus"},{"location":"software/tools/rufus/#wat-is-rufus","text":"(https://rufus.ie/pics/rufus_en.png) Rufus is a utility that helps format and create bootable USB flash drives, such as USB keys/pendrives, memory sticks, etc. It can be especially useful for cases where: you need to create USB installation media from bootable ISOs (Windows, Linux, UEFI, etc.) you need to work on a system that doesn't have an OS installed you need to flash a BIOS or other firmware from DOS you want to run a low-level utility Despite its small size, Rufus provides everything you need! Oh, and Rufus is fast . For instance it's about twice as fast as UNetbootin , Universal USB Installer or Windows 7 USB download tool , on the creation of a Windows 7 USB installation drive from an ISO. It is also marginally faster on the creation of Linux bootable USB from ISOs. A non exhaustive list of Rufus supported ISOs is also provided at the bottom of this page.","title":"Wat is Rufus?"},{"location":"software/tools/usb_installation_media/","text":"USB Installation Media \u00b6 Generic USB Tools \u00b6 BalenaEtcher Ventoy Rufus Raspberry Pi USB Tools \u00b6 Raspberry Pi Imager","title":"USB Installation media"},{"location":"software/tools/usb_installation_media/#usb-installation-media","text":"","title":"USB Installation Media"},{"location":"software/tools/usb_installation_media/#generic-usb-tools","text":"BalenaEtcher Ventoy Rufus","title":"Generic USB Tools"},{"location":"software/tools/usb_installation_media/#raspberry-pi-usb-tools","text":"Raspberry Pi Imager","title":"Raspberry Pi USB Tools"},{"location":"software/tools/ventoy/","text":"Ventoy \u00b6 Wat is Ventoy? \u00b6 Ventoy is an open source tool to create bootable USB drive for ISO/WIM/IMG/VHD(x)/EFI files. With ventoy, you don't need to format the disk over and over, you just need to copy the ISO/WIM/IMG/VHD(x)/EFI files to the USB drive and boot them directly. You can copy many files at a time and ventoy will give you a boot menu to select them ( screenshot ). You can also browse ISO/WIM/IMG/VHD(x)/EFI files in local disks and boot them. x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI, ARM64 UEFI and MIPS64EL UEFI are supported in the same way. Most types of OS supported (Windows/WinPE/Linux/ChromeOS/Unix/VMware/Xen...) links \u00b6 Website : https://www.ventoy.net/en/index.html Download : https://www.ventoy.net/en/download.html","title":"Ventoy"},{"location":"software/tools/ventoy/#ventoy","text":"","title":"Ventoy"},{"location":"software/tools/ventoy/#wat-is-ventoy","text":"Ventoy is an open source tool to create bootable USB drive for ISO/WIM/IMG/VHD(x)/EFI files. With ventoy, you don't need to format the disk over and over, you just need to copy the ISO/WIM/IMG/VHD(x)/EFI files to the USB drive and boot them directly. You can copy many files at a time and ventoy will give you a boot menu to select them ( screenshot ). You can also browse ISO/WIM/IMG/VHD(x)/EFI files in local disks and boot them. x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI, ARM64 UEFI and MIPS64EL UEFI are supported in the same way. Most types of OS supported (Windows/WinPE/Linux/ChromeOS/Unix/VMware/Xen...)","title":"Wat is Ventoy?"},{"location":"software/tools/ventoy/#links","text":"Website : https://www.ventoy.net/en/index.html Download : https://www.ventoy.net/en/download.html","title":"links"}]}